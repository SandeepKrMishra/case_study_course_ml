{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohit\\Anaconda3\\envs\\py35\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1631</td>\n",
       "      <td>hate</td>\n",
       "      <td>My stomach is killing me do j can't sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8481</td>\n",
       "      <td>anger</td>\n",
       "      <td>@anz_rocks19  i have to break the twitterparty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>763</td>\n",
       "      <td>hate</td>\n",
       "      <td>Taking back the HORRIBLE shoes my mum made me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>396</td>\n",
       "      <td>sadness</td>\n",
       "      <td>my stupid tooooth hurts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18851</td>\n",
       "      <td>anger</td>\n",
       "      <td>too sick for rigging tomorrow.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 sentiment                                            content\n",
       "0        1631      hate          My stomach is killing me do j can't sleep\n",
       "1        8481     anger  @anz_rocks19  i have to break the twitterparty...\n",
       "2         763      hate  Taking back the HORRIBLE shoes my mum made me ...\n",
       "3         396   sadness                            my stupid tooooth hurts\n",
       "4       18851     anger                     too sick for rigging tomorrow."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_json('D:/dataset/Yahoo QA Topic Classification.json', lines=True)\n",
    "data =pd.read_csv(\"C:/Users/mohit/Desktop/NN/sentiment_data_5l.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding =open('D:/word_embedding/pre_trained_gloVe/glove.6B/glove.6B.50d.txt',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glovefile):\n",
    "    word_list = []\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    word_to_vec_map={}\n",
    "    index = 0\n",
    "    for line in glovefile: \n",
    "        word =line.split(\" \")\n",
    "        word_vec_list =np.float_(word[1:51])\n",
    "        word_list.append(word[0])\n",
    "        word_to_vec_map[word[0]] =word_vec_list\n",
    "        word_to_index[word[0]] = index\n",
    "        index = index + 1\n",
    "        \n",
    "    index_to_word = dict(zip(word_to_index.values(), word_to_index.keys()))\n",
    "    \n",
    "    return (word_to_index, index_to_word, word_to_vec_map, word_list)\n",
    "\n",
    "def cosine_similarity(u,v):\n",
    "    \n",
    "    dot=np.dot(u,v)\n",
    "    u_mod=np.sqrt(np.sum(np.multiply(u,u)))\n",
    "    v_mod=np.sqrt(np.sum(np.multiply(v,v)))\n",
    "    \n",
    "    similarity =dot/(u_mod * v_mod)\n",
    "    return similarity\n",
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \n",
    "    sentence_lower = sentence.lower()\n",
    "    sentence_words = nltk.word_tokenize(sentence_lower)\n",
    "    \n",
    "    avg = np.zeros((50,))\n",
    "    sent_len_counter = 1\n",
    "    for word in sentence_words:\n",
    "        if not word in stop_words:\n",
    "            if word_to_vec_map.get(word) is not None:\n",
    "                avg += word_to_vec_map.get(word)\n",
    "                sent_len_counter = sent_len_counter + 1\n",
    "        \n",
    "    avg = avg/sent_len_counter\n",
    "    \n",
    "    return avg\n",
    "        \n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / e_z.sum(axis=0)\n",
    "\n",
    "def predict(X, Y_oh, W, b, word_to_vec_map, lebal_encoder_class):\n",
    "    \n",
    "    m = Y_oh.shape[0]\n",
    "    predict = []\n",
    "    for sentence_index in range(m):\n",
    "        avg = sentence_to_avg(X[sentence_index], word_to_vec_map)   \n",
    "        z = np.dot(W,avg)+b\n",
    "        a = softmax(z)\n",
    "        list_values = list(a)\n",
    "        pred_index = list_values.index(max(list_values))\n",
    "        pred_sentiment = lebal_encoder_class[pred_index]\n",
    "        predict.append(pred_sentiment)\n",
    "    return predict \n",
    "\n",
    "def predict1(X, W, b, word_to_vec_map, lebal_encoder_class):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    predict = []\n",
    "    for sentence_index in range(m):\n",
    "        avg = sentence_to_avg(X[sentence_index], word_to_vec_map)   \n",
    "        z = np.dot(W,avg)+b\n",
    "        a = softmax(z)\n",
    "        list_values = list(a)\n",
    "        pred_index = list_values.index(max(list_values))\n",
    "        pred_sentiment = lebal_encoder_class[pred_index]\n",
    "        predict.append(pred_sentiment)\n",
    "    return predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'happiness' 'hate' 'love' 'sadness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohit\\Anaconda3\\envs\\py35\\lib\\site-packages\\sklearn\\preprocessing\\label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\mohit\\Anaconda3\\envs\\py35\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "train_data_le_y = le.fit_transform(train_data[[\"sentiment\"]])\n",
    "test_data_le_y = le.transform(test_data[[\"sentiment\"]])\n",
    "le_classes = le.classes_\n",
    "print(le_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[2 2 3 3 0 4 2 4 0 2 4 3 2 4 0 3 0 3 4 4 4 3 4 2 4 2 3 1 1 2 1 2 3 0 0 3 3\n",
      " 3 1 0 3 0 1 0 1 0 0 0 1 3 3 3 0 2 4 0 2 3 0 4 0 1 0 1 2 2 4 4 0 0 1 4 1 3\n",
      " 0 0 0 1 4 1 4 3 4 0 2 1 1 4 1 4 0 0 0 0 3 3 4 0 2 1 0 4 3 1 4 3 1 1 0 3 4\n",
      " 1 1 3 1 2 3 4 3 1 1 3 1 0 0 3 4 2 4 0 4 1 0 1 0 0 1 1 3 4 0 4 4 2 0 0 2 1\n",
      " 2 2 4 2 2 3 3 3 0 2 1 2 0 3 4 2 1 3 3 0 4 2 0 4 2 2 1 3 0 0 0 4 2 4 4 2 0\n",
      " 3 2 4 2 1 3 2 3 2 4 1 0 0 1 0 1 3 0 1 3 0 1 4 2 0 1 0 4 2 3 2 1 0 3 3 4 0\n",
      " 0 4 1 1 4 0 2 0 3 0 4 1 0 3 3 4 3 1 4 2 2 2 3 3 4 4 1 0 4 1 0 3 2 1 1 4 0\n",
      " 1 4 4 2 2 1 2 2 3 4 3 3 2 4 3 3 0 1 3 1 3 3 4 0 4 2 3 3 1 3 1 0 4 4 4 2 3\n",
      " 2 1 0 3 2 1 2 4 2 2 2 1 4 1 3 0 2 1 3 4 1 0 2 4 1 1 1 4 3 1 1 4 1 0 2 3 1\n",
      " 0 4 4 1 4 1 3 4 3 0 0 1 2 1 2 2 2 2 0 4 2 0 4 4 1 4 2 1 1 2 4 0 4 4 3 1 2\n",
      " 3 4 0 0 0 0 1 0 2 3 2 2 0 3 0 2 2 3 2 2 3 2 3 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data_le_y))\n",
    "print(train_data_le_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map, word_list = read_glove_vecs(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "400000\n",
      "400000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_index))\n",
    "print(len(index_to_word))\n",
    "print(len(word_to_vec_map))\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of krishna in the vocabulary is 12782\n",
      "the 12782th word in the vocabulary is krishna\n"
     ]
    }
   ],
   "source": [
    "word = \"krishna\"\n",
    "index = 12782\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y_oh, word_to_vec_map, lebal_encoder_class, learning_rate = 0.01, num_iterations = 400):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Define number of training examples\n",
    "    m = Y_oh.shape[0]                          # number of training examples\n",
    "    n_y = Y_oh.shape[1]                                 # number of classes  \n",
    "    n_h = 50                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    #Y_oh = convert_to_one_hot(Y, C = n_y)\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        \n",
    "        for sentence_index in range(m):\n",
    "            avg = sentence_to_avg(X[sentence_index], word_to_vec_map)\n",
    "            \n",
    "            z = np.dot(W,avg)+b\n",
    "            a = softmax(z)\n",
    "            \n",
    "             \n",
    "            cost = -np.sum(np.multiply(Y_oh[sentence_index], np.log(a)))\n",
    "            \n",
    "            \n",
    "            dz = a - Y_oh[sentence_index]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y_oh, W, b, word_to_vec_map, lebal_encoder_class)\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 2.173863787748305\n",
      "Epoch: 100 --- cost = 0.30368059361537286\n",
      "Epoch: 200 --- cost = 0.13781914469884504\n",
      "Epoch: 300 --- cost = 0.08622735005880636\n"
     ]
    }
   ],
   "source": [
    "X_train = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
    " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
    " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
    " 'You totally deserve this prize', 'Let us go play football',\n",
    " 'Are you down for football this afternoon', 'Work hard play harder',\n",
    " 'It is suprising how people can be dumb sometimes',\n",
    " 'I am very disappointed','It is the best day in my life',\n",
    " 'I think I will end up alone','My life is so boring','Good job',\n",
    " 'Great so awesome'])\n",
    "\n",
    "Y_train = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
    "Y_hot_array = pd.get_dummies(pd.Series(Y_train)).values\n",
    "\n",
    "le_classes_demo = ['fun', \"love\", \"miss\", \"enthusiasm\", \"sad\", \"worry\"]\n",
    "\n",
    "pred, W, b = model(X_train, Y_hot_array, word_to_vec_map, le_classes_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_Y_hot_array = pd.get_dummies(pd.Series(train_data_le_y)).values\n",
    "test_data_Y_hot_array = pd.get_dummies(pd.Series(test_data_le_y)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.9722075841367954\n",
      "Epoch: 100 --- cost = 0.13943084030253142\n",
      "Epoch: 200 --- cost = 0.10314076067129022\n",
      "Epoch: 300 --- cost = 0.09405915179524556\n"
     ]
    }
   ],
   "source": [
    "pred_data, W_parm, b_param = model(train_data[\"content\"].values, train_data_Y_hot_array, word_to_vec_map, le_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_values = predict(test_data[\"content\"].values, test_data_Y_hot_array, W_parm, b_param, word_to_vec_map, le_classes)\n",
    "actual_test_data_values = test_data[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_values1 = predict1(X_my_sentences, W_parm, b_param, word_to_vec_map, le_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happiness', 'love', 'love', 'love', 'anger', 'sadness']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_data_values1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hate', 'sadness', 'love', 'sadness', 'love', 'love', 'happiness',\n",
       "       'anger', 'hate', 'happiness', 'anger', 'anger', 'happiness',\n",
       "       'love', 'happiness', 'love', 'happiness', 'anger', 'hate', 'love',\n",
       "       'hate', 'happiness', 'hate', 'hate', 'happiness', 'hate',\n",
       "       'happiness', 'happiness', 'happiness', 'sadness', 'love',\n",
       "       'happiness', 'sadness', 'hate', 'hate', 'love', 'love', 'love',\n",
       "       'hate', 'love', 'anger', 'anger', 'hate', 'sadness', 'happiness',\n",
       "       'anger', 'anger', 'hate', 'love', 'hate', 'sadness', 'hate',\n",
       "       'love', 'sadness', 'love', 'love', 'hate', 'anger', 'sadness',\n",
       "       'love', 'love', 'hate', 'love', 'sadness', 'anger', 'happiness',\n",
       "       'sadness', 'love', 'happiness', 'sadness', 'hate', 'happiness',\n",
       "       'sadness', 'happiness', 'anger', 'happiness', 'anger', 'hate',\n",
       "       'sadness', 'love', 'hate', 'sadness', 'sadness', 'hate',\n",
       "       'happiness', 'hate', 'hate', 'sadness', 'hate', 'love', 'anger',\n",
       "       'happiness', 'sadness', 'sadness', 'happiness', 'anger', 'sadness',\n",
       "       'sadness', 'anger'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_test_data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'love',\n",
       " 'love',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'love',\n",
       " 'hate',\n",
       " 'love',\n",
       " 'anger',\n",
       " 'love',\n",
       " 'sadness',\n",
       " 'happiness',\n",
       " 'hate',\n",
       " 'sadness',\n",
       " 'sadness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'hate',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'hate',\n",
       " 'hate',\n",
       " 'love',\n",
       " 'hate',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'sadness',\n",
       " 'hate',\n",
       " 'happiness',\n",
       " 'hate',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'anger',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'love',\n",
       " 'sadness',\n",
       " 'happiness',\n",
       " 'hate',\n",
       " 'sadness',\n",
       " 'hate',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'anger',\n",
       " 'hate',\n",
       " 'anger',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'love',\n",
       " 'hate',\n",
       " 'happiness',\n",
       " 'love',\n",
       " 'love',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'hate',\n",
       " 'love',\n",
       " 'happiness',\n",
       " 'love',\n",
       " 'love',\n",
       " 'love',\n",
       " 'hate',\n",
       " 'hate',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'hate',\n",
       " 'sadness',\n",
       " 'hate',\n",
       " 'sadness',\n",
       " 'anger',\n",
       " 'love',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'sadness',\n",
       " 'hate',\n",
       " 'hate',\n",
       " 'love',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'hate',\n",
       " 'love',\n",
       " 'anger',\n",
       " 'hate',\n",
       " 'love',\n",
       " 'anger',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'love',\n",
       " 'happiness',\n",
       " 'love',\n",
       " 'anger',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'sadness']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
