{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "import gensim\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen_sentence = 40\n",
    "\n",
    "\n",
    "categories = ['rec.autos','rec.sport.hockey','comp.graphics','sci.space']    \n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=0, categories=categories, remove=('headers','footers','quotes'))\n",
    "train_corpus = dataset.data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loding word2vec model and creating word vector representation of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohit\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "sentences = [[word for word in doc.lower().translate(str.maketrans('','',string.punctuation)).split()[:maxLen_sentence]] for doc in train_corpus]\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preaparing data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_sentences = np.zeros([len(sentences), maxLen_sentence], dtype=np.int32)\n",
    "train_y_sentences = np.zeros([len(sentences)], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if(len(sentence) != 0):\n",
    "        for t, word in enumerate(sentence[:-1]):\n",
    "            train_x_sentences[i, t] = word_model.wv.vocab[word].index\n",
    "        train_y_sentences[i] = word_model.wv.vocab[sentence[-1]].index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sequentia\n",
    "l model of word embedding and LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting model with traing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.6101\n",
      "Epoch 2/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.5220\n",
      "Epoch 3/20\n",
      "2371/2371 [==============================] - 23s 10ms/step - loss: 6.4918\n",
      "Epoch 4/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.4719\n",
      "Epoch 5/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.4563\n",
      "Epoch 6/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.4456\n",
      "Epoch 7/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.4267\n",
      "Epoch 8/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.4131\n",
      "Epoch 9/20\n",
      "2371/2371 [==============================] - 23s 10ms/step - loss: 6.3939\n",
      "Epoch 10/20\n",
      "2371/2371 [==============================] - 23s 10ms/step - loss: 6.3790\n",
      "Epoch 11/20\n",
      "2371/2371 [==============================] - 23s 10ms/step - loss: 6.3558\n",
      "Epoch 12/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.3365\n",
      "Epoch 13/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.3178\n",
      "Epoch 14/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.2955\n",
      "Epoch 15/20\n",
      "2371/2371 [==============================] - 22s 9ms/step - loss: 6.2744\n",
      "Epoch 16/20\n",
      "2371/2371 [==============================] - 23s 10ms/step - loss: 6.2528\n",
      "Epoch 17/20\n",
      "2371/2371 [==============================] - 23s 10ms/step - loss: 6.2298\n",
      "Epoch 18/20\n",
      "2371/2371 [==============================] - 23s 10ms/step - loss: 6.2083\n",
      "Epoch 19/20\n",
      "2371/2371 [==============================] - 23s 10ms/step - loss: 6.1850\n",
      "Epoch 20/20\n",
      "2371/2371 [==============================] - 23s 10ms/step - loss: 6.1610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4570801320>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x_sentences, train_y_sentences, batch_size=128, epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting next word on test 'text' data - predict_post_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.1669643e-04 4.8426763e-04 4.8619957e-04 ... 6.7238718e-05\n",
      "  6.7682442e-05 6.7055356e-05]] 11574\n",
      "a 1 locates 6502\n",
      "[[1.2330088e-04 1.5929976e-04 1.7735061e-04 ... 8.1959130e-05\n",
      "  8.3250525e-05 8.7663640e-05]] 11574\n",
      "little 175 your 64\n",
      "[[2.7769667e-04 3.3828052e-04 3.9021831e-04 ... 7.3552204e-05\n",
      "  7.3666801e-05 7.4901342e-05]] 11574\n",
      "more 72 varieties 6961\n",
      "[[2.1878858e-04 2.3636142e-04 2.0960893e-04 ... 7.8828023e-05\n",
      "  7.8989971e-05 7.8178113e-05]] 11574\n",
      "and 5 walk 2094\n",
      "[[6.1669643e-04 4.8426763e-04 4.8619957e-04 ... 6.7238718e-05\n",
      "  6.7682442e-05 6.7055356e-05]] 11574\n",
      "a 1 quality 1083\n",
      "[[1.2330088e-04 1.5929976e-04 1.7735061e-04 ... 8.1959130e-05\n",
      "  8.3250525e-05 8.7663640e-05]] 11574\n",
      "little 175 manually 4944\n",
      "[[1.0381615e-04 1.3200955e-04 1.5198936e-04 ... 8.3133440e-05\n",
      "  8.4218744e-05 8.7206754e-05]] 11574\n",
      "bit 233 mechanic 3807\n",
      "[[1.5490265e-04 1.4213531e-04 1.4408700e-04 ... 8.2217135e-05\n",
      "  8.3342507e-05 8.3257233e-05]] 11574\n",
      "smarter 7612 televised 6119\n",
      "[[3.1033049e-03 1.3283639e-03 1.3652286e-03 ... 3.7148850e-05\n",
      "  3.7349862e-05 3.6658112e-05]] 11574\n",
      "in 6 copious 6810\n",
      "[[1.0520652e-03 6.4308947e-04 6.2461238e-04 ... 6.2494015e-05\n",
      "  5.6526762e-05 6.0549442e-05]] 11574\n",
      "his 90 letting 3294\n",
      "[[2.6081581e-04 2.3279856e-04 2.2889892e-04 ... 7.8651028e-05\n",
      "  8.4417188e-05 7.8179182e-05]] 11574\n",
      "own 317 irresponsibility 10673\n",
      "[[6.1205612e-04 4.5354883e-04 4.7941844e-04 ... 6.5863176e-05\n",
      "  7.0821603e-05 6.9935108e-05]] 11574\n",
      "end 459 1457 5999\n"
     ]
    }
   ],
   "source": [
    "pre_text = 'A little more caution, and a little bit smarter in his own end'\n",
    "for word in text.lower().split():\n",
    "        if type(word_model.wv.vocab.get(word)) is not type(None):\n",
    "            word_idxs = word_model.wv.vocab[word].index\n",
    "            x=np.array(word_idxs)\n",
    "            x.shape =  (1, )\n",
    "            prediction = model.predict(x)\n",
    "            print(prediction, len(prediction[0]))\n",
    "            \n",
    "            preds = np.asarray(prediction).astype('float64')\n",
    "            preds = np.log(preds)\n",
    "            exp_preds = np.exp(preds)\n",
    "            preds = exp_preds / np.sum(exp_preds)\n",
    "            probas = np.random.multinomial(1, preds.flatten(), 1)\n",
    "            pred_idx = np.argmax(probas)\n",
    "\n",
    "            pred_word = word_model.wv.index2word[pred_idx]\n",
    "            print(word, word_idxs, pred_word, pred_idx)\n",
    "\n",
    "#prediction = model.predict(x=np.array(word_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
